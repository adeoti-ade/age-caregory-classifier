{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "import PIL.Image\n",
    "opt = SGD(lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\"train/\")\n",
    "val_dir = Path(\"validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(train_dir.glob(\"*/*.jpeg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18964"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(train_dir.glob(\"children/*.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = list(train_dir.glob(\"adult/*.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=48x48 at 0x7F314FF76E80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIL.Image.open(str(children[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('train/children/children_5109.jpeg')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "children[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "img_height = 48\n",
    "img_width = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18965 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3318 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "val_ds = train_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.Resizing(32, 32),\n",
    "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3,3), padding='same', activation=\"relu\", input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPool2D(2,2),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = opt,\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 46, 46, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 23, 23, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 23, 23, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 8464)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 512)               4334080   \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 4,499,397\n",
      "Trainable params: 4,499,397\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "593/593 [==============================] - 24s 40ms/step - loss: 2.9583 - accuracy: 0.3321 - val_loss: 2.7863 - val_accuracy: 0.4096\n",
      "Epoch 2/20\n",
      "593/593 [==============================] - 24s 41ms/step - loss: 2.7471 - accuracy: 0.4453 - val_loss: 2.6987 - val_accuracy: 0.4473\n",
      "Epoch 3/20\n",
      "593/593 [==============================] - 23s 39ms/step - loss: 2.5977 - accuracy: 0.4974 - val_loss: 2.5080 - val_accuracy: 0.5202\n",
      "Epoch 4/20\n",
      "593/593 [==============================] - 23s 39ms/step - loss: 2.4927 - accuracy: 0.5261 - val_loss: 2.4484 - val_accuracy: 0.5208\n",
      "Epoch 5/20\n",
      "593/593 [==============================] - 24s 41ms/step - loss: 2.4141 - accuracy: 0.5458 - val_loss: 2.3807 - val_accuracy: 0.5401\n",
      "Epoch 6/20\n",
      "593/593 [==============================] - 23s 39ms/step - loss: 2.3509 - accuracy: 0.5627 - val_loss: 2.3246 - val_accuracy: 0.5452\n",
      "Epoch 7/20\n",
      "593/593 [==============================] - 23s 39ms/step - loss: 2.3003 - accuracy: 0.5676 - val_loss: 2.2876 - val_accuracy: 0.5615\n",
      "Epoch 8/20\n",
      "593/593 [==============================] - 22s 37ms/step - loss: 2.2537 - accuracy: 0.5772 - val_loss: 2.3174 - val_accuracy: 0.5277\n",
      "Epoch 9/20\n",
      "593/593 [==============================] - 22s 38ms/step - loss: 2.2066 - accuracy: 0.5880 - val_loss: 2.2055 - val_accuracy: 0.5699\n",
      "Epoch 10/20\n",
      "593/593 [==============================] - 22s 38ms/step - loss: 2.1610 - accuracy: 0.5950 - val_loss: 2.1944 - val_accuracy: 0.5561\n",
      "Epoch 11/20\n",
      "593/593 [==============================] - 23s 38ms/step - loss: 2.1193 - accuracy: 0.6038 - val_loss: 2.1999 - val_accuracy: 0.5416\n",
      "Epoch 12/20\n",
      "593/593 [==============================] - 23s 38ms/step - loss: 2.0759 - accuracy: 0.6122 - val_loss: 2.1350 - val_accuracy: 0.5711\n",
      "Epoch 13/20\n",
      "593/593 [==============================] - 22s 38ms/step - loss: 2.0301 - accuracy: 0.6179 - val_loss: 2.0763 - val_accuracy: 0.5787\n",
      "Epoch 14/20\n",
      "593/593 [==============================] - 22s 38ms/step - loss: 1.9885 - accuracy: 0.6276 - val_loss: 2.0561 - val_accuracy: 0.5817\n",
      "Epoch 15/20\n",
      "593/593 [==============================] - 23s 40ms/step - loss: 1.9489 - accuracy: 0.6327 - val_loss: 2.0375 - val_accuracy: 0.5651\n",
      "Epoch 16/20\n",
      "593/593 [==============================] - 25s 42ms/step - loss: 1.9057 - accuracy: 0.6404 - val_loss: 2.0098 - val_accuracy: 0.5856\n",
      "Epoch 17/20\n",
      "593/593 [==============================] - 26s 45ms/step - loss: 1.8623 - accuracy: 0.6515 - val_loss: 2.0101 - val_accuracy: 0.5714\n",
      "Epoch 18/20\n",
      "593/593 [==============================] - 25s 43ms/step - loss: 1.8210 - accuracy: 0.6596 - val_loss: 1.9637 - val_accuracy: 0.5871\n",
      "Epoch 19/20\n",
      "593/593 [==============================] - 26s 43ms/step - loss: 1.7789 - accuracy: 0.6707 - val_loss: 1.9428 - val_accuracy: 0.5859\n",
      "Epoch 20/20\n",
      "257/593 [============>.................] - ETA: 14s - loss: 1.7393 - accuracy: 0.6845"
     ]
    }
   ],
   "source": [
    "model.fit(train_ds, validation_data=val_ds, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
